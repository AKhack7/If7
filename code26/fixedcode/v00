
import pyttsx3
import speech_recognition as sr
import datetime
import os
import webbrowser
import pyautogui
import time
import psutil
import pywhatkit
import random
import subprocess
import re
import threading
import socket
import glob
import logging
from sympy import sympify, sin, cos, tan, sqrt, pi
import urllib.request
import cv2
import numpy as np
import requests
import ctypes
import queue
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs

# === Optional Offline STT (Vosk) ===
try:
    import json
    from vosk import Model as VoskModel, KaldiRecognizer
    VOSK_AVAILABLE = True
except Exception:
    VOSK_AVAILABLE = False

# === Optional MediaPipe ===
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except Exception:
    MEDIAPIPE_AVAILABLE = False

# Initialize logging
logging.basicConfig(filename="isha_assistant.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")


class IshaAssistant:
    """A personal desktop assistant with voice and text command capabilities."""
    def __init__(self):
        self.engine = pyttsx3.init()
        self.engine.setProperty('rate', 150)
        self.engine.setProperty('volume', 0.9)

        # Initialize speech recognition
        self.recognizer = sr.Recognizer()
        self.recognizer.dynamic_energy_threshold = True
        self.microphone = None

        # Offline STT config (Vosk)
        self.vosk_model = None
        self.vosk_recognizer = None
        self.vosk_sample_rate = 16000
        self.VOSK_MODEL_PATH = os.environ.get("VOSK_MODEL_PATH", "").strip()

        self._init_vosk_if_possible()

        try:
            self.microphone = sr.Microphone()
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
        except (AttributeError, OSError, sr.RequestError) as e:
            logging.error(f"Microphone initialization failed: {str(e)}")
            message = "Voice recognition disabled. PyAudio not found or microphone issue. Please install PyAudio and check microphone."
            self.speak(message)
            print(message)
            self.select_microphone()

        self.is_listening = False

        # Internet check caching
        self.last_internet_check = 0
        self.internet_status = False
        self.internet_check_interval = 10

        # Set female voice
        self.set_female_voice()

        # Settings and Apps lists (unchanged mappings)
        self.SETTING_MAP = {
            "display setting": ("ms-settings:display", "01"),
            "sound setting": ("ms-settings:sound", "02"),
            "notification & action setting": ("ms-settings:notifications", "03"),
            "focus assist setting": ("ms-settings:quiethours", "04"),
            "power & sleep setting": ("ms-settings:powersleep", "05"),
            "storage setting": ("ms-settings:storagesense", "06"),
            "tablet setting": ("ms-settings:tablet", "07"),
            "multitasking setting": ("ms-settings:multitasking", "08"),
            "projecting to this pc setting": ("ms-settings:project", "09"),
            "shared experiences setting": ("ms-settings:crossdevice", "010"),
            "system components setting": ("ms-settings:appsfeatures-app", "001"),
            "clipboard setting": ("ms-settings:clipboard", "002"),
            "remote desktop setting": ("ms-settings:remotedesktop", "003"),
            "optional features setting": ("ms-settings:optionalfeatures", "004"),
            "about setting": ("ms-settings:about", "005"),
            "system setting": ("ms-settings:system", "006"),
            "devices setting": ("ms-settings:devices", "007"),
            "mobile devices setting": ("ms-settings:mobile-devices", "008"),
            "network & internet setting": ("ms-settings:network", "009"),
            "personalization setting": ("ms-settings:personalization", "000"),
            "apps setting": ("ms-settings:appsfeatures", "10"),
            "account setting": ("ms-settings:yourinfo", "20"),
            "time & language setting": ("ms-settings:dateandtime", "30"),
            "gaming setting": ("ms-settings:gaming", "40"),
            "ease of access setting": ("ms-settings:easeofaccess", "50"),
            "privacy setting": ("ms-settings:privacy", "60"),
            "updated & security": ("ms-settings:windowsupdate", "70")
        }

        self.SETTING_MAP4s = {
            "01": ("ms-settings:display"),
            "02": ("ms-settings:sound"),
            "03": ("ms-settings:notifications"),
            "04": ("ms-settings:quiethours"),
            "05": ("ms-settings:powersleep"),
            "06": ("ms-settings:storagesense"),
            "07": ("ms-settings:tablet"),
            "08": ("ms-settings:multitasking"),
            "09": ("ms-settings:project"),
            "010": ("ms-settings:crossdevice"),
            "001": ("ms-settings:appsfeatures-app"),
            "002": ("ms-settings:clipboard"),
            "003": ("ms-settings:remotedesktop"),
            "004": ("ms-settings:optionalfeatures"),
            "005": ("ms-settings:about"),
            "006": ("ms-settings:system"),
            "007": ("ms-settings:devices"),
            "008": ("ms-settings:mobile-devices"),
            "009": ("ms-settings:network"),
            "000": ("ms-settings:personalization"),
            "10": ("ms-settings:appsfeatures"),
            "20": ("ms-settings:yourinfo"),
            "30": ("ms-settings:dateandtime"),
            "40": ("ms-settings:gaming"),
            "50": ("ms-settings:easeofaccess"),
            "60": ("ms-settings:privacy"),
            "70": ("ms-settings:windowsupdate")
        }

        self.apps_commands = {
            "alarms & clock": ("ms-clock:", "a1"),
            "calculator": ("calc", "c1"),
            "calendar": ("outlookcal:", "c2"),
            "camera": ("microsoft.windows.camera:", "c3"),
            "copilot": ("ms-copilot:", "c4"),
            "cortana": ("ms-cortana:", "c5"),
            "game bar": ("ms-gamebar:", "gb1"),
            "groove music": ("mswindowsmusic:", "gm1"),
            "mail": ("outlookmail:", "m1"),
            "maps": ("bingmaps:", "ms1"),
            "microsoft edge": ("msedge", "me1"),
            "microsoft solitaire collection": ("ms-solitaire:", "mc1"),
            "microsoft store": ("ms-windows-store:", "ms1"),
            "mixed reality portal": ("ms-mixedreality:", "mp1"),
            "movies & tv": ("mswindowsvideo:", "mt1"),
            "office": ("ms-office:", "o1"),
            "onedrive": ("ms-onedrive:", "oe"),
            "onenote": ("ms-onenote:", "oe"),
            "outlook": ("outlookmail:", "ouk"),
            "outlook (classic)": ("ms-outlook:", "oc1"),
            "paint": ("mspaint", "p1"),
            "paint 3d": ("ms-paint:", "p3d"),
            "phone link": ("ms-phonelink:", "pk"),
            "power point": ("ms-powerpoint:", "pt"),
            "settings": ("ms-settings:", "ss"),
            "skype": ("skype:", "sk1"),
            "snip & sketch": ("ms-snip:", "s0h"),
            "sticky note": ("ms-stickynotes:", "s1e"),
            "tips": ("ms-tips:", "ts0"),
            "voice recorder": ("ms-soundrecorder:", "vr0"),
            "weather": ("msnweather:", "w1"),
            "windows backup": ("ms-settings:backup", "wb1"),
            "windows security": ("ms-settings:windowsdefender", "ws1"),
            "word": ("ms-word:", "wrd"),
            "xbox": ("ms-xbox:", "xb"),
            "about your pc": ("ms-settings:about", "apc")
        }

        self.apps_commands4q = {
            "a1": "ms-clock:",
            "c1": "calc",
            "c2": "outlookcal:",
            "c3": "microsoft.windows.camera:",
            "c4": "ms-copilot:",
            "c5": "ms-cortana:",
            "gb1": "ms-gamebar:",
            "gm1": "mswindowsmusic:",
            "m1": "outlookmail:",
            "ms1": "bingmaps:",
            "me1": "msedge",
            "mc1": "ms-solitaire:",
            "ms1": "ms-windows-store:",
            "mp1": "ms-mixedreality:",
            "mt1": "mswindowsvideo:",
            "o1": "ms-office:",
            "oe": "ms-onedrive:",
            "oe": "ms-onenote:",
            "ouk": "outlookmail:",
            "oc1": "ms-outlook:",
            "p1": "mspaint",
            "p3d": "ms-paint:",
            "pk": "ms-phonelink:",
            "pt": "ms-powerpoint:",
            "ss": "ms-settings:",
            "sk1": "skype:",
            "s0h": "ms-snip:",
            "s1e": "ms-stickynotes:",
            "ts0": "ms-tips:",
            "vr0": "ms-soundrecorder:",
            "w1": "msnweather:",
            "wb1": "ms-settings:backup",
            "ws1": "ms-settings:windowsdefender",
            "wrd": "ms-word:",
            "xb": "ms-xbox:",
            "apc": "ms-settings:about"
        }

        self.software_dict = {
            "notepad": "notepad",
            "ms word": "winword",
            "command prompt": "cmd",
            "excel": "excel",
            "vscode": "code",
            "word16": "winword",
            "file explorer": "explorer",
            "edge": "msedge",
            "microsoft 365 copilot": "ms-copilot:",
            "outlook": "outlook",
            "microsoft store": "ms-windows-store:",
            "photos": "microsoft.photos:",
            "xbox": "xbox:",
            "solitaire": "microsoft.microsoftsolitairecollection:",
            "clipchamp": "clipchamp",
            "to do": "microsoft.todos:",
            "linkedin": "https://www.linkedin.com",
            "calculator": "calc",
            "news": "bingnews:",
            "one drive": "onedrive",
            "onenote 2016": "onenote",
            "google": "https://www.google.com"
        }

        self.commands_dict = {**self.SETTING_MAP, **self.SETTING_MAP4s, **self.software_dict, **self.apps_commands, **self.apps_commands4q}
        self.commands_dict = {k: v if isinstance(v, str) else v[0] for k, v in self.commands_dict.items()}
        self.settings_display_to_cmd = {f"{name} ({code})": cmd for name, (cmd, code) in self.SETTING_MAP.items()}
        self.apps_display_to_cmd = {name: cmd for name, cmd in self.apps_commands.items()}

        self.input_queue = queue.Queue()
        self.pending = None

        # Gesture mode state
        self.gesture_thread = None
        self.gesture_running = False
        self.file_handle = None  # for pointing gesture open/close

        self.wish_me()
        self.start_server()

    # -------------------- OFFLINE STT (Vosk) --------------------
    def _init_vosk_if_possible(self):
        try:
            if not VOSK_AVAILABLE:
                logging.info("Vosk not available.")
                return
            if not self.VOSK_MODEL_PATH or not os.path.isdir(self.VOSK_MODEL_PATH):
                logging.info("VOSK_MODEL_PATH not set or invalid. Offline STT disabled.")
                return
            self.vosk_model = VoskModel(self.VOSK_MODEL_PATH)
            self.vosk_recognizer = KaldiRecognizer(self.vosk_model, self.vosk_sample_rate)
            logging.info("Vosk offline STT initialized.")
        except Exception as e:
            logging.error(f"Failed to init Vosk: {e}")
            self.vosk_model = None
            self.vosk_recognizer = None

    def _listen_vosk(self, duration_sec=6):
        """Offline voice recognition via Vosk (no internet)."""
        if not self.vosk_recognizer or not self.microphone:
            return None
        try:
            import pyaudio
            pa = pyaudio.PyAudio()
            stream = pa.open(format=pyaudio.paInt16, channels=1, rate=self.vosk_sample_rate,
                             input=True, frames_per_buffer=4000)
            stream.start_stream()
            start = time.time()
            text = ""
            while time.time() - start < duration_sec:
                data = stream.read(4000, exception_on_overflow=False)
                if self.vosk_recognizer.AcceptWaveform(data):
                    res = json.loads(self.vosk_recognizer.Result())
                    text = res.get("text", "") or text
            res = json.loads(self.vosk_recognizer.FinalResult())
            text = res.get("text", "") or text
            stream.stop_stream()
            stream.close()
            pa.terminate()
            return text.strip().lower() if text else None
        except Exception as e:
            logging.error(f"Vosk listen failed: {e}")
            return None

    # -------------------- SERVER UI --------------------
    def start_server(self):
        class CustomHandler(BaseHTTPRequestHandler):
            def do_GET(self):
                parsed_path = urlparse(self.path)
                query_params = parse_qs(parsed_path.query)
                path = parsed_path.path

                if path == '/':
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html')
                    self.end_headers()
                    self.wfile.write(self.server.assistant.get_html().encode())
                elif path == '/command':
                    cmd = query_params.get('cmd', [None])[0]
                    if cmd is None:
                        self.send_response(400)
                        self.end_headers()
                        return
                    if self.server.assistant.pending:
                        self.server.assistant.input_queue.put(cmd)
                        self.send_response(200)
                        self.send_header('Content-type', 'text/plain')
                        self.end_headers()
                        self.wfile.write(b'Input received')
                    else:
                        response = self.server.assistant.process_command(cmd)
                        self.send_response(200)
                        self.send_header('Content-type', 'text/plain')
                        self.end_headers()
                        self.wfile.write(response.encode())
                elif path == '/voice':
                    self.server.assistant.toggle_voice()
                    message = "Microphone toggled"
                    self.send_response(200)
                    self.send_header('Content-type', 'text/plain')
                    self.end_headers()
                    self.wfile.write(message.encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def log_message(self, format, *args):
                pass

        server = HTTPServer(('localhost', 8000), CustomHandler)
        server.assistant = self
        threading.Thread(target=server.serve_forever, daemon=True).start()
        webbrowser.open('http://localhost:8000/')

    def get_html(self):
        # UI kept same as your style; only minimal change: placeholder mention activet 156
        apps_html = ''.join(f'<div class="app-item" data-command="open {name}" style="margin:6px 0; cursor:pointer;">â€¢ {name}</div>' for name in sorted(self.apps_display_to_cmd.keys()))
        settings_html = ''.join(f'<div class="setting-item" data-command="open {name}" style="margin:6px 0; cursor:pointer;">â€¢ {name}</div>' for name in sorted(self.SETTING_MAP.keys()))
        html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>ISHA Assistant</title>
<style>
  :root{{
    --bg1:#050519;
    --bg2:#0f1636;
    --neon1:#00e0ff;
    --neon2:#7b4bff;
    --glass: rgba(255,255,255,0.04);
  }}
  *{{box-sizing:border-box; -webkit-font-smoothing:antialiased; font-family: "Segoe UI", Inter, system-ui, sans-serif}}
  html,body{{height:100%; margin:0; background: linear-gradient(180deg,var(--bg1),var(--bg2)); color:#e8f6ff; display:flex; align-items:center; justify-content:center; overflow:hidden;}}
  .container {{
    width: 540px; max-width:calc(100% - 40px); height: 680px;
    border-radius: 24px; position: relative; padding: 30px;
    background: linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01));
    border: 1px solid rgba(255,255,255,0.04);
    box-shadow: 0 20px 60px rgba(5,10,40,0.6);
    overflow: hidden;
    backdrop-filter: blur(8px);
  }}
  .topbar {{ display:flex; align-items:center; justify-content:space-between; gap:12px; }}
  .title {{ font-weight:700; font-size:18px; color:#cfeeff; display:flex; align-items:center; gap:10px; }}
  .title .dot{{ width:12px; height:12px; border-radius:50%; background: linear-gradient(135deg,var(--neon1),var(--neon2)); }}
  .stage {{ width:100%; height:420px; display:flex; align-items:center; justify-content:center; }}
  .core {{ width:220px; height:220px; border-radius:50%; display:flex; align-items:center; justify-content:center;
           background: radial-gradient(circle at 40% 30%, rgba(255,255,255,0.14), rgba(0,224,255,0.07));
           border:1px solid rgba(0,224,255,0.15); }}
  .label {{ font-weight:800; font-size:28px; letter-spacing:2px; color:#e9fbff; }}
  .datetime {{ margin-top:18px; text-align:center; color:var(--neon1); font-weight:700; font-size:16px; }}
  .datetime .time {{ font-size:28px; color:#dffbff; }}
  .controls {{ margin-top:24px; width:100%; display:flex; align-items:center; gap:12px; }}
  .input {{
    flex:1; height:48px; border-radius:14px; padding:10px 16px;
    background:var(--glass); border:1px solid rgba(255,255,255,0.04);
    color:#e8f6ff; outline:none; font-size:15px;
  }}
  .icon-btn {{
    width:48px; height:48px; border-radius:12px; border:none;
    background: linear-gradient(180deg, #0c1228, #05051a);
    color:var(--neon1); cursor:pointer; font-size:18px;
    border:1px solid rgba(0,224,255,0.06);
  }}
  .popup {{
    position: absolute; width:320px; min-height:120px; border-radius:12px; padding:14px;
    background: rgba(10,12,22,0.98);
    border:1px solid rgba(0,224,255,0.06);
    display:none; z-index:20; cursor: move;
    transition: all 0.4s ease;
  }}
  #appPopup {{ left:-340px; top:120px; }}
  #settingsPopup {{ right:-340px; top:140px; }}
  #appPopup.active {{ left:40px; }}
  #settingsPopup.active {{ right:40px; }}
  .popup .head {{ display:flex; justify-content:space-between; align-items:center; gap:8px; margin-bottom:8px; color:var(--neon1); font-weight:700; }}
  .popup .close {{
    width:34px; height:34px; border-radius:8px; display:inline-grid; place-items:center; background: rgba(255,255,255,0.02); cursor:pointer;
    color:#fff; border:1px solid rgba(255,255,255,0.03);
  }}
</style>
</head>
<body>
  <div class="container" id="container">
    <div class="topbar">
      <div class="title"><span class="dot"></span> ISHA Assistant</div>
      <div style="opacity:0.8; font-size:13px; color:#bfefff">Type: activet 156</div>
    </div>

    <div class="stage">
      <div class="core"><div class="label">ISHA</div></div>
    </div>

    <div class="datetime" id="datetime">
      <div class="time" id="time">00:00:00</div>
      <div class="date" id="date">Loading date...</div>
    </div>

    <div class="controls">
      <input class="input" id="cmd" placeholder="Type command (e.g., activet 156)..." />
      <div class="right">
        <button class="icon-btn" id="appBtn" title="Apps">A</button>
        <button class="icon-btn" id="settingsBtn" title="Settings">S</button>
        <button class="icon-btn" id="voiceBtn" title="Voice">V</button>
      </div>
    </div>
  </div>

  <div class="popup" id="appPopup">
    <div class="head"><div>Applications</div><div class="close" data-close="appPopup">âœ•</div></div>
    <div style="font-size:14px; color:#cfeeff;">
        {apps_html}
    </div>
  </div>

  <div class="popup" id="settingsPopup">
    <div class="head"><div>Settings</div><div class="close" data-close="settingsPopup">âœ•</div></div>
    <div style="font-size:14px; color:#cfeeff;">
        {settings_html}
    </div>
  </div>

<script>
function updateDateTime(){{
  const now = new Date();
  const timeEl = document.getElementById('time');
  const dateEl = document.getElementById('date');
  timeEl.textContent = now.toLocaleTimeString([], {{ hour:'2-digit', minute:'2-digit', second:'2-digit', hour12:false }});
  dateEl.textContent = now.toLocaleDateString([], {{ weekday:'short', year:'numeric', month:'short', day:'numeric' }});
}}
setInterval(updateDateTime, 500);
updateDateTime();

function makeDraggable(el){{
  let dragging=false, ox=0, oy=0;
  el.addEventListener('mousedown', (e)=>{{
    dragging=true;
    ox = e.clientX - el.offsetLeft;
    oy = e.clientY - el.offsetTop;
    el.style.transition = 'none';
    document.body.style.userSelect='none';
  }});
  window.addEventListener('mousemove', (e)=>{{
    if(!dragging) return;
    el.style.left = (e.clientX - ox) + 'px';
    el.style.top = (e.clientY - oy) + 'px';
  }});
  window.addEventListener('mouseup', ()=>{{
    if(dragging){{
      dragging=false;
      el.style.transition = '';
      document.body.style.userSelect='';
    }}
  }});
}}

const appBtn = document.getElementById('appBtn');
const settingsBtn = document.getElementById('settingsBtn');
const voiceBtn = document.getElementById('voiceBtn');
const appPopup = document.getElementById('appPopup');
const settingsPopup = document.getElementById('settingsPopup');

appBtn.addEventListener('click', () => {{
  const isActive = appPopup.classList.toggle('active');
  if (isActive) {{
    appPopup.style.display = 'block';
    settingsPopup.classList.remove('active');
    settingsPopup.style.display = 'none';
  }} else {{
    setTimeout(()=> appPopup.style.display = 'none', 200);
  }}
}});

settingsBtn.addEventListener('click', () => {{
  const isActive = settingsPopup.classList.toggle('active');
  if (isActive) {{
    settingsPopup.style.display = 'block';
    appPopup.classList.remove('active');
    appPopup.style.display = 'none';
  }} else {{
    setTimeout(()=> settingsPopup.style.display = 'none', 200);
  }}
}});

document.querySelectorAll('.close').forEach(btn=>{{
  btn.addEventListener('click', () => {{
    const id = btn.dataset.close;
    if(id) document.getElementById(id).classList.remove('active');
    setTimeout(()=> document.getElementById(id).style.display='none', 200);
  }});
}});

makeDraggable(appPopup);
makeDraggable(settingsPopup);

document.querySelectorAll('.app-item').forEach(item => {{
  item.addEventListener('click', () => {{
    const cmd = item.getAttribute('data-command');
    sendCommand(cmd);
    appPopup.classList.remove('active');
    setTimeout(() => appPopup.style.display = 'none', 200);
  }});
}});
document.querySelectorAll('.setting-item').forEach(item => {{
  item.addEventListener('click', () => {{
    const cmd = item.getAttribute('data-command');
    sendCommand(cmd);
    settingsPopup.classList.remove('active');
    setTimeout(() => settingsPopup.style.display = 'none', 200);
  }});
}});

function sendCommand(cmd) {{
  fetch(`/command?cmd=${{encodeURIComponent(cmd)}}`)
  .then(res => res.text())
  .then(text => console.log('Response:', text))
  .catch(err => console.error('Error:', err));
}}

document.getElementById('cmd').addEventListener('keydown', (e)=>{{
  if(e.key === 'Enter'){{
    const v = e.target.value.trim();
    if(!v) return;
    sendCommand(v);
    e.target.value = '';
  }}
}});

voiceBtn.addEventListener('click', ()=>{{
  fetch('/voice').then(res => res.text()).then(_ => {{}});
}});
</script>
</body>
</html>
        """
        return html

    # -------------------- MIC SELECTION --------------------
    def select_microphone(self):
        """Allow user to select a microphone from available devices."""
        try:
            mic_names = sr.Microphone.list_microphone_names()
            if not mic_names:
                message = "No microphones detected. Please check your hardware and permissions."
                self.speak(message)
                print(message)
                return

            mic_list = "\n".join([f"{i}: {name}" for i, name in enumerate(mic_names)])
            selected = self.user_input(f"Available microphones:\n{mic_list}\nEnter the index number of the microphone to use:")

            if selected is not None:
                try:
                    index = int(selected)
                    if 0 <= index < len(mic_names):
                        self.microphone = sr.Microphone(device_index=index)
                        with self.microphone as source:
                            self.recognizer.adjust_for_ambient_noise(source, duration=2)
                        message = f"Selected microphone: {mic_names[index]}. Voice recognition enabled."
                        self.speak(message)
                        print(message)
                    else:
                        message = "Invalid microphone index. Please try again."
                        self.speak(message)
                        print(message)
                except ValueError:
                    message = "Invalid input. Please enter a number."
                    self.speak(message)
                    print(message)
                except (OSError, sr.RequestError) as e:
                    message = f"Failed to initialize selected microphone: {str(e)}. Try another one."
                    self.speak(message)
                    print(message)
        except Exception as e:
            message = f"Error accessing microphone list: {str(e)}. Ensure PyAudio is installed and microphone permissions are granted."
            self.speak(message)
            print(message)

    def user_input(self, prompt):
        console = ctypes.windll.kernel32.GetConsoleWindow()
        if console:
            ctypes.windll.user32.ShowWindow(console, 5)  # Show console
        res = input(prompt)
        if console:
            ctypes.windll.user32.ShowWindow(console, 0)  # Hide again
        return res

    def set_female_voice(self):
        """Set the TTS engine to use a female voice, with fallback and logging."""
        try:
            voices = self.engine.getProperty('voices')
            selected_voice = None

            logging.info("Available voices: %s", [voice.name for voice in voices])

            for voice in voices:
                if "zira" in voice.name.lower() or "female" in voice.name.lower():
                    selected_voice = voice
                    break

            if not selected_voice:
                selected_voice = voices[0] if voices else None
                logging.warning("No female voice found, falling back to default voice")
                message = "No female voice available, using default voice."
                self.speak(message)
            else:
                logging.info("Selected female voice: %s", selected_voice.name)

            if selected_voice:
                self.engine.setProperty('voice', selected_voice.id)
                self.engine.say("Initializing voice")
                self.engine.runAndWait()
            else:
                logging.error("No voices available for text-to-speech")
                message = "No voices available for text-to-speech. Please check system TTS settings."
                self.speak(message)
        except Exception as e:
            logging.error(f"Failed to set female voice: {str(e)}")
            message = "Failed to initialize text-to-speech. Please check audio drivers or TTS installation."
            self.speak(message)

    # -------------------- INTERNET CHECK (kept) --------------------
    def check_internet(self):
        current_time = time.time()
        if current_time - self.last_internet_check < self.internet_check_interval:
            return self.internet_status

        self.last_internet_check = current_time
        for host in [("8.8.8.8", 80), ("1.1.1.1", 80)]:
            try:
                socket.create_connection(host, timeout=2)
                self.internet_status = True
                return True
            except (socket.gaierror, socket.timeout):
                continue
        self.internet_status = False
        return False

    # -------------------- VOICE TOGGLE --------------------
    def toggle_voice(self):
        if self.microphone is None:
            self.select_microphone()
            if self.microphone is None:
                message = "Voice recognition is disabled due to missing dependencies or hardware. Use text input instead."
                self.speak(message)
                print(message)
                return
        self.is_listening = not self.is_listening
        if self.is_listening:
            message = "Microphone is now on"
            self.speak(message)
            print(message)
            threading.Thread(target=self.listen_voice, daemon=True).start()
        else:
            message = "Microphone is now off"
            self.speak(message)
            print(message)

    def wish_me(self):
        current_hour = datetime.datetime.now().hour
        greeting = (
            "Good morning" if 5 <= current_hour < 12 else
            "Good afternoon" if 12 <= current_hour < 17 else
            "Good evening" if 17 <= current_hour < 21 else
            "Good night"
        )
        self.speak(greeting)
        time.sleep(1)
        message = "I am Isha, Intelligent System for Human Assistance. Welcome!"
        self.speak(message)
        time.sleep(2)

    # -------------------- LISTEN (offline-first) --------------------
    def listen(self):
        # Prefer offline Vosk if available
        if self.vosk_recognizer:
            text = self._listen_vosk(duration_sec=6)
            if text:
                return text

        # Fallback to SpeechRecognition (may use online Google if configured)
        if self.microphone is None:
            self.select_microphone()
            if self.microphone is None:
                query = self.user_input("Voice not available. Enter your command: ")
                return query.lower() if query else None

        try:
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source, duration=2)
                for _ in range(3):
                    try:
                        audio = self.recognizer.listen(source, timeout=10, phrase_time_limit=10)
                        # IMPORTANT: recognize_google is online. If offline-only required, rely on Vosk.
                        return self.recognizer.recognize_google(audio).lower()
                    except sr.WaitTimeoutError:
                        self.speak("No speech detected, retrying...")
                        continue
                    except sr.UnknownValueError:
                        self.speak("Could not understand, retrying...")
                        continue
                    except sr.RequestError as e:
                        logging.error(f"Google STT error: {e}")
                        query = self.user_input("Voice STT failed. Enter your command: ")
                        return query.lower() if query else None
            query = self.user_input("Voice input failed. Enter your command: ")
            return query.lower() if query else None
        except Exception as e:
            logging.error(f"Voice input failed: {e}")
            query = self.user_input("Voice not available. Enter your command: ")
            return query.lower() if query else None

    def listen_voice(self):
        while self.is_listening:
            command = self.listen()
            if command:
                self.process_command(command)
            time.sleep(0.5)

    # -------------------- COMMAND ROUTING --------------------
    def open_google(self):
        try:
            webbrowser.open("https://www.google.com")
            message = "Opening Google"
            self.speak(message)
            return message
        except Exception as e:
            message = f"Failed to open Google: {str(e)}"
            self.speak(message)
            return message

    def handle_settings_apps_commands(self, command):
        cmd = None
        if command in self.commands_dict:
            cmd = self.commands_dict[command]
        elif command in self.settings_display_to_cmd:
            cmd = self.settings_display_to_cmd[command]
        elif command in self.apps_display_to_cmd:
            cmd = self.apps_display_to_cmd[command]

        if cmd:
            try:
                if cmd.startswith("http"):
                    webbrowser.open(cmd)
                else:
                    subprocess.run(["start", "", cmd], shell=True)
                message = f"Opening {command}"
                self.speak(message)
                return message
            except Exception as e:
                message = f"Failed to open {command}: {str(e)}"
                self.speak(message)
                return message
        return None

    def process_command(self, command):
        logging.info(f"Processing command: {command}, Internet: {self.internet_status}")
        command = command.lower().strip()

        if self.pending:
            self.input_queue.put(command)
            return "Input received for pending request."

        # NEW: gesture activation command
        if command == "activet 156":
            return self.start_gesture_mode()

        if command.startswith("open "):
            app_or_setting = command[5:].strip()
            result = self.handle_settings_apps_commands(app_or_setting)
            if result:
                return result

        if command in ["what is the time", "tell me the time", "current time", "time now", "what time is it", "what's the time", "time", "what time"]:
            message = datetime.datetime.now().strftime("%H:%M:%S")
            self.speak(message)
            return message

        elif command in ["what is the date", "tell me the date", "current date", "date now", "what date is it", "what's the date", "date", "what date"]:
            message = datetime.datetime.now().strftime("%A, %B %d, %Y")
            self.speak(message)
            return message

        # Offline mode: keep internet features optional
        elif command in ["open google", "launch google", "go to google"]:
            if self.check_internet():
                return self.open_google()
            else:
                message = "No internet connection. Google cannot be opened."
                self.speak(message)
                return message

        elif command in ["weather", "check weather", "what's the weather"]:
            # Offline-only requirement: keep cached only
            return self.get_weather_offline_first()

        elif command in ["hi", "hello", "hey"]:
            message = "Hello! How can I assist you today?"
            self.speak(message)
            return message

        message = f"Command not recognized: {command}"
        self.speak(message)
        return message

    # -------------------- OFFLINE-FIRST WEATHER --------------------
    def get_weather_offline_first(self):
        message = "Which city's weather do you want to check?"
        self.speak(message)
        self.pending = 'weather_city'
        city = self.input_queue.get()
        self.pending = None

        if not city or city.lower() in ["none", "cancel", "no"]:
            message = "No city provided. Please try again."
            self.speak(message)
            return message

        # If internet available, fetch; else cached only
        if self.check_internet():
            try:
                response = requests.get(f"https://wttr.in/{city}?format=%C+%t", timeout=5)
                response.raise_for_status()
                weather_info = response.text.strip()
                with open("weather_cache.txt", "w", encoding="utf-8") as f:
                    f.write(f"{city}:{weather_info}:{int(time.time())}")
                message = f"Weather in {city}: {weather_info}"
                self.speak(message)
                return message
            except Exception as e:
                logging.error(f"Weather fetch failed: {e}")

        # cached fallback
        try:
            with open("weather_cache.txt", "r", encoding="utf-8") as f:
                cache_data = f.read().strip()
            if not cache_data:
                message = "No internet and no cached weather available."
                self.speak(message)
                return message
            c_city, weather_info, timestamp = cache_data.split(":", 2)
            age = int(time.time()) - int(timestamp)
            if age < 3600:
                message = f"No internet. Showing cached weather for {c_city}: {weather_info}"
            else:
                message = "No internet and cached weather is too old."
            self.speak(message)
            return message
        except Exception as e:
            message = f"No internet and no valid cached weather available: {str(e)}."
            self.speak(message)
            return message

    # -------------------- GESTURE MODE (MediaPipe) --------------------
    def start_gesture_mode(self):
        if not MEDIAPIPE_AVAILABLE:
            message = "MediaPipe is not installed. Please install mediapipe to use gesture mode."
            self.speak(message)
            return message

        if self.gesture_running:
            message = "Gesture mode already running."
            self.speak(message)
            return message

        self.gesture_running = True
        self.gesture_thread = threading.Thread(target=self._gesture_loop, daemon=True)
        self.gesture_thread.start()
        message = "Gesture mode activated. Show your palm to the camera."
        self.speak(message)
        return message

    def _unique_filename(self, prefix="capture", ext="png"):
        now = datetime.datetime.now()
        # Unique name: dd-mm-yyyy__HH-MM-SS__ms
        return f"{prefix}__{now.strftime('%d-%m-%Y__%H-%M-%S')}__{int(now.microsecond/1000):03d}.{ext}"

    def _ensure_folder(self, folder):
        try:
            os.makedirs(folder, exist_ok=True)
        except Exception:
            pass

    def _gesture_loop(self):
        mp_hands = mp.solutions.hands
        mp_draw = mp.solutions.drawing_utils

        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            self.speak("Camera not available.")
            self.gesture_running = False
            return

        self.speak("Camera opened. Press Q in the camera window to exit gesture mode.")

        palm_seen_since = None
        icon_visible = False
        last_action_time = 0

        # For your file open/close snippet
        file_handle = None

        with mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.6,
            min_tracking_confidence=0.6
        ) as hands:

            while self.gesture_running:
                ret, frame = cap.read()
                if not ret:
                    break

                frame = cv2.flip(frame, 1)
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                result = hands.process(rgb)

                gesture = None
                landmarks = None

                if result.multi_hand_landmarks:
                    hand_landmarks = result.multi_hand_landmarks[0]
                    landmarks = hand_landmarks.landmark
                    mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                    if self._is_open_palm(landmarks):
                        gesture = "PALM"
                    elif self._is_closed_fist(landmarks):
                        gesture = "FIST"
                    elif self._is_pointing_gesture(landmarks):
                        gesture = "POINT"

                # UI overlay
                cv2.rectangle(frame, (10, 10), (520, 110), (0, 0, 0), -1)
                cv2.putText(frame, "ISHA Gesture Mode (activet 156)", (20, 40),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                cv2.putText(frame, f"Gesture: {gesture}", (20, 80),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

                nowt = time.time()

                # (3) Pointing -> open file, Fist -> close file (your snippet idea)
                if gesture == "POINT":
                    if file_handle is None:
                        try:
                            file_handle = open("my_data.txt", "w", encoding="utf-8")
                            file_handle.write("File opened with hand gesture!\n")
                            self.speak("File opened by pointing gesture.")
                            logging.info("File opened by pointing gesture.")
                        except Exception as e:
                            logging.error(f"File open failed: {e}")
                elif gesture == "FIST":
                    if file_handle is not None:
                        try:
                            file_handle.close()
                            file_handle = None
                            self.speak("File closed by fist gesture.")
                            logging.info("File closed by fist gesture.")
                        except Exception as e:
                            logging.error(f"File close failed: {e}")

                # (1) Palm away -> icon appears -> fist => screenshot
                # Here: if PALM stays for >1.0 sec we show ICON, then FIST triggers screenshot
                if gesture == "PALM":
                    if palm_seen_since is None:
                        palm_seen_since = nowt
                    if (nowt - palm_seen_since) >= 1.0:
                        icon_visible = True
                else:
                    palm_seen_since = None
                    icon_visible = False

                if icon_visible:
                    cv2.circle(frame, (470, 60), 18, (0, 255, 0), -1)
                    cv2.putText(frame, "ICON", (420, 100),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

                # Rate-limit actions
                if nowt - last_action_time > 1.5:
                    # Screenshot rule: icon visible + fist
                    if icon_visible and gesture == "FIST":
                        try:
                            folder = os.path.join(os.getcwd(), "isha_captures")
                            self._ensure_folder(folder)
                            fname = self._unique_filename(prefix="screenshot", ext="png")
                            path = os.path.join(folder, fname)
                            shot = pyautogui.screenshot()
                            shot.save(path)
                            self.speak("Screenshot saved.")
                            logging.info(f"Screenshot saved: {path}")
                            last_action_time = nowt
                            icon_visible = False
                            palm_seen_since = None
                        except Exception as e:
                            logging.error(f"Screenshot failed: {e}")

                    # (2) Selfie: PALM then FIST => save camera frame
                    # Simple heuristic: if PALM seen recently and now FIST, save frame
                    if gesture == "FIST" and palm_seen_since is not None and (nowt - palm_seen_since) < 1.2:
                        try:
                            folder = os.path.join(os.getcwd(), "isha_captures")
                            self._ensure_folder(folder)
                            fname = self._unique_filename(prefix="selfie", ext="jpg")
                            path = os.path.join(folder, fname)
                            cv2.imwrite(path, frame)
                            self.speak("Selfie captured.")
                            logging.info(f"Selfie saved: {path}")
                            last_action_time = nowt
                        except Exception as e:
                            logging.error(f"Selfie failed: {e}")

                cv2.imshow("ISHA - Gesture Mode", frame)
                key = cv2.waitKey(1) & 0xFF
                if key in [ord('q'), ord('Q')]:
                    break

        try:
            if file_handle is not None:
                file_handle.close()
        except Exception:
            pass

        cap.release()
        cv2.destroyAllWindows()
        self.gesture_running = False
        self.speak("Gesture mode stopped.")

    # -------------------- BASIC GESTURE HEURISTICS --------------------
    def _dist(self, a, b):
        return ((a.x - b.x) ** 2 + (a.y - b.y) ** 2) ** 0.5

    def _is_closed_fist(self, lm):
        # fingertips close to palm center (approx wrist/index_mcp)
        wrist = lm[0]
        index_mcp = lm[5]
        palm_center = index_mcp
        tips = [lm[4], lm[8], lm[12], lm[16], lm[20]]
        d = sum(self._dist(t, palm_center) for t in tips) / len(tips)
        return d < 0.18

        # PINCH VOLUME ðŸ¤
            thumb = (int(lm[4].x * screen_w), int(lm[4].y * screen_h))
            index = (int(lm[8].x * screen_w), int(lm[8].y * screen_h))

            d = distance(thumb, index)
            if d < 40:
                if prev_pinch_x:
                    if index[0] > prev_pinch_x + 20:
                        volume_up()
                    elif index[0] < prev_pinch_x - 20:
                        volume_down()
                prev_pinch_x = index[0]
            else:
                prev_pinch_x = None


            # INDEX FINGER â†’ MOUSE MOVE
            ix = int(lm[8].x * screen_w)
            iy = int(lm[8].y * screen_h)
            pyautogui.moveTo(ix, iy)

            # HOVER CLICK (2 sec)
            if hover_click.check(2):
                pyautogui.click()

            # HOVER OPEN (4 sec)
            if hover_open.check(4):
                pyautogui.doubleClick()

def match_pattern(self, text):
        text = text.lower().strip()
        words = text.split()

        bigrams = [" ".join(words[i:i+2]) for i in range(len(words)-1)]
        trigrams = [" ".join(words[i:i+3]) for i in range(len(words)-2)]

        patterns = {
            "greeting": [r"hello", r"hi", r"namaste", r"hey"],
            "bye": [r"bye", r"tata", r"see you"],
            "howAreYou": [r"kaise ho", r"how are you", r"kya haal"],
            "time": [r"time", r"samay"],
            "date": [r"date", r"tarikh"],
            "playMusic": [r"play song", r"gaana", r"music"],
            "math": [r"\d+.*[\+\-\*\/].*\d+"],
        }

        for key, regs in patterns.items():
            for r in regs:
                if any(re.search(r, x) for x in trigrams + bigrams + words):
                    return key
        return "unknown"


    def _is_open_palm(self, lm):
        # fingertips far from palm center
        index_mcp = lm[5]
        palm_center = index_mcp
        tips = [lm[4], lm[8], lm[12], lm[16], lm[20]]
        d = sum(self._dist(t, palm_center) for t in tips) / len(tips)
        return d > 0.28

    def _is_pointing_gesture(self, lm):
        # index extended, others curled
        index_tip = lm[8]
        index_pip = lm[6]
        middle_tip = lm[12]
        ring_tip = lm[16]
        pinky_tip = lm[20]
        wrist = lm[0]

        index_extended = index_tip.y < index_pip.y  # higher up in image
        others_down = (middle_tip.y > index_pip.y and ring_tip.y > index_pip.y and pinky_tip.y > index_pip.y)
        # also ensure index is somewhat away from wrist
        return index_extended and others_down and (self._dist(index_tip, wrist) > 0.25)

    # -------------------- SAFE "SELF-IMPROVE" (no self-modify) --------------------
    def suggest_improvements_from_log(self, last_lines=200):
        """
        Safe mode: reads logs and suggests improvements (does NOT modify source code).
        """
        try:
            if not os.path.exists("isha_assistant.log"):
                return "No log file found yet."
            with open("isha_assistant.log", "r", encoding="utf-8", errors="ignore") as f:
                lines = f.readlines()[-last_lines:]
            errors = [ln for ln in lines if "ERROR" in ln or "Failed" in ln]
            if not errors:
                return "No recent errors found in logs."
            return "Recent issues detected:\n" + "".join(errors[-20:])
        except Exception as e:
            return f"Could not analyze logs: {e}"

    # -------------------- SPEAK --------------------
    def speak(self, text):
        def run_speak():
            try:
                self.engine.stop()
                time.sleep(0.2)
                if getattr(self.engine, "_inLoop", False):
                    logging.warning("TTS engine is busy, attempting to reinitialize: %s", text)
                    try:
                        self.engine.endLoop()
                    except Exception:
                        pass
                    time.sleep(0.3)
                    self.engine = pyttsx3.init()
                    self.set_female_voice()
                self.engine.say(text)
                self.engine.runAndWait()
                logging.info(f"Successfully spoke: {text}")
            except Exception as e:
                logging.error(f"Speech error: {str(e)} - Text: {text}")

        threading.Thread(target=run_speak, daemon=True).start()
        time.sleep(0.2)


if __name__ == "__main__":
    try:
        console = ctypes.windll.kernel32.GetConsoleWindow()
        if console:
            ctypes.windll.user32.ShowWindow(console, 0)
        app = IshaAssistant()
        while True:
            time.sleep(1)
    except Exception as e:
        logging.error(f"Application failed to start: {str(e)}")
        print(f"Error: Application failed to start: {str(e)}")

        

fixe thise code
